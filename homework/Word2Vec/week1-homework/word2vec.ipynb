{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 'PAD'\n",
    "GO_TOKEN = 'GO'\n",
    "EOS_TOKEN = 'EOS'\n",
    "UNK_TOKEN = 'UNK'\n",
    "\n",
    "start_id = 0\n",
    "end_id = 1\n",
    "unk_id = 2\n",
    "\n",
    "\n",
    "def save_word_dict(dict_data, save_path):\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for k, v in dict_data.items():\n",
    "            f.write(\"%s\\t%d\\n\" % (k, v))\n",
    "\n",
    "\n",
    "def read_vocab(input_texts, max_size=50000, min_count=5):\n",
    "    token_counts = Counter()\n",
    "    special_tokens = [PAD_TOKEN, GO_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "    for line in input_texts:\n",
    "        for char in line.strip():\n",
    "            char = char.strip()\n",
    "            if not char:\n",
    "                continue\n",
    "            token_counts.update(char)\n",
    "    # Sort word count by value\n",
    "    count_pairs = token_counts.most_common()\n",
    "    vocab = [k for k, v in count_pairs if v >= min_count]\n",
    "    # Insert the special tokens to the beginning\n",
    "    vocab[0:0] = special_tokens\n",
    "    full_token_id = list(zip(vocab, range(len(vocab))))[:max_size]\n",
    "    vocab2id = dict(full_token_id)\n",
    "    return vocab2id\n",
    "\n",
    "\n",
    "def stat_dict(lines):\n",
    "    word_dict = {}\n",
    "    for line in lines:\n",
    "        tokens = line.split(\" \")\n",
    "        for t in tokens:\n",
    "            t = t.strip()\n",
    "            if t:\n",
    "                word_dict[t] = word_dict.get(t, 0) + 1\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def filter_dict(word_dict, min_count=3):\n",
    "    out_dict = copy.deepcopy(word_dict)\n",
    "    for w,c in out_dict.items():\n",
    "        if c < min_count:\n",
    "            del out_dict[w]\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def read_lines(path, col_sep=None):\n",
    "    lines = []\n",
    "    with open(path, mode='r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if col_sep:\n",
    "                if col_sep in line:\n",
    "                    lines.append(line)\n",
    "            else:\n",
    "                lines.append(line)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_dict(dict_path):\n",
    "    return dict((line.strip().split(\"\\t\")[0], idx)\n",
    "                for idx, line in enumerate(open(dict_path, \"r\", encoding='utf-8').readlines()))\n",
    "\n",
    "\n",
    "def load_reverse_dict(dict_path):\n",
    "    return dict((idx, line.strip().split(\"\\t\")[0])\n",
    "                for idx, line in enumerate(open(dict_path, \"r\", encoding='utf-8').readlines()))\n",
    "\n",
    "\n",
    "def flatten_list(nest_list):\n",
    "    \"\"\"\n",
    "    嵌套列表压扁成一个列表\n",
    "    :param nest_list: 嵌套列表\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for item in nest_list:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(flatten_list(item))\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "\n",
    "def map_item2id(items, vocab, max_len, non_word=0, lower=False):\n",
    "    \"\"\"\n",
    "    将word/pos等映射为id\n",
    "    :param items: list，待映射列表\n",
    "    :param vocab: 词表\n",
    "    :param max_len: int，序列最大长度\n",
    "    :param non_word: 未登录词标号，默认0\n",
    "    :param lower: bool，小写\n",
    "    :return: np.array, dtype=int32,shape=[max_len,]\n",
    "    \"\"\"\n",
    "    assert type(non_word) == int\n",
    "    arr = np.zeros((max_len,), dtype='int32')\n",
    "    # 截断max_len长度的items\n",
    "    min_range = min(max_len, len(items))\n",
    "    for i in range(min_range):\n",
    "        item = items[i] if not lower else items[i].lower()\n",
    "        arr[i] = vocab[item] if item in vocab else non_word\n",
    "    return arr\n",
    "\n",
    "\n",
    "def write_vocab(vocab, filename):\n",
    "    \"\"\"Writes a vocab to a file\n",
    "    Writes one word per line.\n",
    "    Args:\n",
    "        vocab: iterable that yields word\n",
    "        filename: path to vocab file\n",
    "    Returns:\n",
    "        write a word per line\n",
    "    \"\"\"\n",
    "    print(\"Writing vocab...\")\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        for word, i in sorted(vocab.items(), key=lambda x: x[1]):\n",
    "            if i != len(vocab) - 1:\n",
    "                f.write(word + '\\n')\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"- write to {} done. {} tokens\".format(filename, len(vocab)))\n",
    "\n",
    "\n",
    "def load_vocab(filename):\n",
    "    \"\"\"Loads vocab from a file\n",
    "    Args:\n",
    "        filename: (string) the format of the file must be one word per line.\n",
    "    Returns:\n",
    "        d: dict[word] = index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = dict()\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            # lines = f.readlines()\n",
    "            for idx, word in enumerate(f.readlines()):\n",
    "                word = word.strip()\n",
    "                d[word] = idx\n",
    "\n",
    "    except IOError:\n",
    "        raise IOError(filename)\n",
    "    return d\n",
    "\n",
    "\n",
    "def transform_data(data, vocab):\n",
    "    # transform sent to ids\n",
    "    out_data = []\n",
    "    for d in data:\n",
    "        tmp_d = []\n",
    "        for sent in d:\n",
    "            tmp_d.append([vocab.get(t, unk_id) for t in sent if t])\n",
    "        out_data.append(tmp_d)\n",
    "    return out_data\n",
    "\n",
    "\n",
    "def load_pkl(pkl_path):\n",
    "    \"\"\"\n",
    "    加载词典文件\n",
    "    :param pkl_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    return result\n",
    "\n",
    "\n",
    "def dump_pkl(vocab, pkl_path, overwrite=True):\n",
    "    \"\"\"\n",
    "    存储文件\n",
    "    :param pkl_path:\n",
    "    :param overwrite:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if pkl_path and os.path.exists(pkl_path) and not overwrite:\n",
    "        return\n",
    "    if pkl_path:\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(vocab, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            # pickle.dump(vocab, f, protocol=0)\n",
    "        print(\"save %s ok.\" % pkl_path)\n",
    "\n",
    "\n",
    "def get_word_segment_data(contents, word_sep=' ', pos_sep='/'):\n",
    "    data = []\n",
    "    for content in contents:\n",
    "        temp = []\n",
    "        for word in content.split(word_sep):\n",
    "            if pos_sep in word:\n",
    "                temp.append(word.split(pos_sep)[0])\n",
    "            else:\n",
    "                temp.append(word.strip())\n",
    "        data.append(word_sep.join(temp))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_char_segment_data(contents, word_sep=' ', pos_sep='/'):\n",
    "    data = []\n",
    "    for content in contents:\n",
    "        temp = ''\n",
    "        for word in content.split(word_sep):\n",
    "            if pos_sep in word:\n",
    "                temp += word.split(pos_sep)[0]\n",
    "            else:\n",
    "                temp += word.strip()\n",
    "        # char seg with list\n",
    "        data.append(word_sep.join(list(temp)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_list(path):\n",
    "    return [word for word in open(path, 'r', encoding='utf-8').read().split()]\n",
    "\n",
    "\n",
    "def save(pred_labels, ture_labels=None, pred_save_path=None, data_set=None):\n",
    "    if pred_save_path:\n",
    "        with open(pred_save_path, 'w', encoding='utf-8') as f:\n",
    "            for i in range(len(pred_labels)):\n",
    "                if ture_labels and len(ture_labels) > 0:\n",
    "                    assert len(ture_labels) == len(pred_labels)\n",
    "                    if data_set:\n",
    "                        f.write(ture_labels[i] + '\\t' + data_set[i] + '\\n')\n",
    "                    else:\n",
    "                        f.write(ture_labels[i] + '\\n')\n",
    "                else:\n",
    "                    if data_set:\n",
    "                        f.write(pred_labels[i] + '\\t' + data_set[i] + '\\n')\n",
    "                    else:\n",
    "                        f.write(pred_labels[i] + '\\n')\n",
    "        print(\"pred_save_path:\", pred_save_path)\n",
    "\n",
    "\n",
    "def load_word2vec(params):\n",
    "    \"\"\"\n",
    "    load pretrain word2vec weight matrix\n",
    "    :param vocab_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word2vec_dict = load_pkl(params['word2vec_output'])\n",
    "    vocab_dict = open(params['vocab_path'], encoding='utf-8').readlines()\n",
    "    embedding_matrix = np.zeros((params['vocab_size'], params['embed_size']))\n",
    "\n",
    "    for line in vocab_dict[:params['vocab_size']]:\n",
    "        word_id = line.split()\n",
    "        word, i = word_id\n",
    "        embedding_vector = word2vec_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[int(i)] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# def get_result_filename(save_result_dir, batch_size, epochs, max_length_inp, embedding_dim, commit=''):\n",
    "def get_result_filename(params, commit=''):\n",
    "    \"\"\"\n",
    "    获取时间\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    save_result_dir = params['test_save_dir']\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    max_length_inp = ['max_dec_len']\n",
    "    embedding_dim = ['embed_size']\n",
    "    now_time = time.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    filename = now_time + '_batch_size_{}_epochs_{}_max_length_inp_{}_embedding_dim_{}{}.csv'.format(batch_size, epochs,\n",
    "                                                                                                     max_length_inp,\n",
    "                                                                                                     embedding_dim,\n",
    "                                                                                                     commit)\n",
    "    result_save_path = os.path.join(save_result_dir, filename)\n",
    "    return result_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(path, col_sep=None):\n",
    "    lines = []\n",
    "    with open(path, mode='r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if col_sep:\n",
    "                if col_sep in line:\n",
    "                    lines.append(line)\n",
    "            else:\n",
    "                lines.append(line)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def extract_sentence(train_x_seg_path, train_y_seg_path, test_seg_path):\n",
    "    ret = []\n",
    "    lines = read_lines(train_x_seg_path)\n",
    "    lines += read_lines(train_y_seg_path)\n",
    "    lines += read_lines(test_seg_path)\n",
    "    for line in lines:\n",
    "        ret.append(line)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def save_sentence(lines, sentence_path):\n",
    "    with open(sentence_path, 'w', encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write('%s\\n' % line.strip())\n",
    "    print('save sentence:%s' % sentence_path)\n",
    "\n",
    "\n",
    "def build(train_x_seg_path, test_y_seg_path, test_seg_path, out_path=None, sentence_path='',\n",
    "          w2v_bin_path=\"w2v.bin\", min_count=1):\n",
    "    sentences = extract_sentence(train_x_seg_path, test_y_seg_path, test_seg_path)\n",
    "    save_sentence(sentences, sentence_path)\n",
    "    print('train w2v model...')\n",
    "    # train model\n",
    "    \"\"\"\n",
    "    通过gensim工具完成word2vec的训练，输入格式采用sentences，使用skip-gram，embedding维度256\n",
    "    your code\n",
    "    w2v = （one line）\n",
    "    \"\"\"\n",
    "    w2v = Word2Vec(sentences,size=256)\n",
    "    w2v.wv.save_word2vec_format(w2v_bin_path, binary=True)\n",
    "    print(\"save %s ok.\" % w2v_bin_path)\n",
    "    # test\n",
    "    sim = w2v.wv.similarity('技师', '车主')\n",
    "    print('技师 vs 车主 similarity score:', sim)\n",
    "    # load model\n",
    "    model = KeyedVectors.load_word2vec_format(w2v_bin_path, binary=True)\n",
    "    word_dict = {}\n",
    "    for word in model.vocab:\n",
    "        word_dict[word] = model[word]\n",
    "    dump_pkl(word_dict, out_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save sentence:./datasets/sentences.txt\n",
      "train w2v model...\n",
      "save w2v.bin ok.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '技师' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-75bce3f452da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0;34m'{}/datasets/test_set.seg_x.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{}/datasets/word2vec.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           sentence_path='{}/datasets/sentences.txt'.format(BASE_DIR))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-be42333f0b8f>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(train_x_seg_path, test_y_seg_path, test_seg_path, out_path, sentence_path, w2v_bin_path, min_count)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"save %s ok.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mw2v_bin_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'技师'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'车主'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'技师 vs 车主 similarity score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \"\"\"\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '技师' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "build('{}/datasets/train_set.seg_x.txt'.format(BASE_DIR),\n",
    "          '{}/datasets/train_set.seg_y.txt'.format(BASE_DIR),\n",
    "          '{}/datasets/test_set.seg_x.txt'.format(BASE_DIR),\n",
    "          out_path='{}/datasets/word2vec.txt'.format(BASE_DIR),\n",
    "          sentence_path='{}/datasets/sentences.txt'.format(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
